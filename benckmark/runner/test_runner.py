#!/usr/bin/env python3
"""
AI Agent ç¼–ç¨‹èƒ½åŠ›æµ‹è¯•é›†åˆ - ä¸»æµ‹è¯•è¿è¡Œå™¨

åŠŸèƒ½:
- ç®¡ç†å’Œæ‰§è¡Œå„çº§åˆ«ç¼–ç¨‹ä»»åŠ¡æµ‹è¯•
- æä¾›å¤šç»´åº¦è¯„ä¼°å’Œè¯„åˆ†
- ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š
- æ”¯æŒå¹¶è¡Œæ‰§è¡Œå’Œæ²™ç®±éš”ç¦»
"""

import argparse
import asyncio
import json
import logging
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

import yaml
from dataclasses import dataclass, asdict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class TestTask:
    """æµ‹è¯•ä»»åŠ¡æ•°æ®ç»“æ„"""

    id: str
    level: str
    domain: str
    title: str
    description: str
    input_spec: Dict[str, Any]
    output_spec: Dict[str, Any]
    evaluation_criteria: Dict[str, Any]
    test_cases: List[Dict[str, Any]]
    time_limit: int = 300  # 5åˆ†é’Ÿé»˜è®¤è¶…æ—¶


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç»“æ„"""

    task_id: str
    success: bool
    score: float
    max_score: float
    execution_time: float
    details: Dict[str, Any]
    error_message: Optional[str] = None
    generated_code: Optional[str] = None


class BenchmarkTestRunner:
    """Benchmarkæµ‹è¯•è¿è¡Œå™¨ä¸»ç±»"""

    def __init__(
        self, config_path: str = "config/test_config.yaml", workspace_path: str = None
    ):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.base_dir = Path(__file__).parent

        # åˆ›å»ºä¸“é—¨çš„ä¸´æ—¶æ–‡ä»¶å¤¹
        self.temp_dir = self.base_dir / "temp_generated"
        self.sandbox_dir = self.temp_dir / "sandbox"
        self.reports_dir = self.base_dir / "reports"
        self.generated_code_dir = self.temp_dir / "generated_code"

        # è®¾ç½®å·¥ä½œåŒºè·¯å¾„
        if workspace_path:
            self.workspace_path = Path(workspace_path)
        else:
            # é»˜è®¤ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•
            self.workspace_path = self.base_dir.parent.parent

        logger.info(f"å·¥ä½œåŒºè·¯å¾„: {self.workspace_path}")
        logger.info(f"ä¸´æ—¶æ–‡ä»¶ç›®å½•: {self.temp_dir}")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.temp_dir.mkdir(exist_ok=True)
        self.sandbox_dir.mkdir(exist_ok=True)
        self.reports_dir.mkdir(exist_ok=True)
        self.generated_code_dir.mkdir(exist_ok=True)

        self.test_tasks: Dict[str, TestTask] = {}
        self.test_results: List[TestResult] = []

        # åŠ è½½å·¥ä½œåŒºé…ç½®
        self.workspace_config = self._load_workspace_config()

        # åˆå§‹åŒ–RAG Agentï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.rag_agent = None
        if self.workspace_config.get("rag_agent", {}).get("enabled", False):
            self._init_rag_agent()

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶ {self.config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "levels": [
                "beginner",
                "elementary",
                "intermediate",
                "advanced",
                "expert",
                "master",
            ],
            "domains": [
                "web_development",
                "mobile_app",
                "algorithms",
                "devops",
                "data_science",
            ],
            "timeout": 300,
            "parallel_tasks": 3,
            "sandbox_enabled": True,
            "output_formats": ["json", "html"],
            "evaluation": {
                "beginner": {
                    "functionality": 40,
                    "code_structure": 25,
                    "readability": 20,
                    "edge_cases": 15,
                },
                "elementary": {
                    "functionality": 35,
                    "code_quality": 25,
                    "user_experience": 20,
                    "error_handling": 20,
                },
                "intermediate": {
                    "functionality": 30,
                    "architecture": 25,
                    "performance": 20,
                    "security": 15,
                    "testing": 10,
                },
                "advanced": {
                    "functionality": 25,
                    "architecture": 25,
                    "performance": 20,
                    "security": 15,
                    "maintainability": 15,
                },
                "expert": {
                    "innovation": 30,
                    "architecture": 25,
                    "performance": 20,
                    "complexity": 15,
                    "knowledge": 10,
                },
                "master": {
                    "innovation": 35,
                    "architecture": 25,
                    "integration": 20,
                    "efficiency": 15,
                    "impact": 5,
                },
            },
        }

    def _load_workspace_config(self) -> Dict[str, Any]:
        """åŠ è½½å·¥ä½œåŒºé…ç½®"""
        workspace_config_path = self.base_dir / "config" / "workspace_config.yaml"
        try:
            if workspace_config_path.exists():
                with open(workspace_config_path, "r", encoding="utf-8") as f:
                    return yaml.safe_load(f)
            else:
                logger.warning(
                    f"å·¥ä½œåŒºé…ç½®æ–‡ä»¶ {workspace_config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®"
                )
                return self._get_default_workspace_config()
        except Exception as e:
            logger.error(f"åŠ è½½å·¥ä½œåŒºé…ç½®å¤±è´¥: {e}")
            return self._get_default_workspace_config()

    def _get_default_workspace_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤å·¥ä½œåŒºé…ç½®"""
        return {
            "workspace": {
                "root_path": str(self.workspace_path),
                "temp_path": str(self.workspace_path / "temp"),
                "rag_data_path": str(self.workspace_path / "temp" / "rag_data"),
                "context_db_path": str(self.workspace_path / "temp" / "contexts.db"),
            },
            "rag_agent": {"enabled": False},
            "environment": {
                "use_project_env": True,
                "working_directory": str(self.workspace_path),
            },
        }

    def _init_rag_agent(self):
        """åˆå§‹åŒ–RAG Agent"""
        try:
            sys.path.insert(0, str(self.workspace_path))
            from src.rag_enhanced_code_agent_workflow import (
                RAGEnhancedCodeAgentWorkflow,
            )

            self.rag_agent = RAGEnhancedCodeAgentWorkflow(
                repo_path=str(self.workspace_path)
            )
            logger.info("âœ… RAG Code Agent åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ RAG Code Agent åˆå§‹åŒ–å¤±è´¥: {e}")
            self.rag_agent = None

    def load_test_tasks(
        self, level: Optional[str] = None, domain: Optional[str] = None
    ):
        """åŠ è½½æµ‹è¯•ä»»åŠ¡"""
        levels_to_load = [level] if level and level != "all" else self.config["levels"]
        domains_to_load = (
            [domain] if domain and domain != "all" else self.config["domains"]
        )

        for level_name in levels_to_load:
            level_dir = self.base_dir / "levels" / level_name
            if level_dir.exists():
                self._load_tasks_from_directory(level_dir, level_name)

        for domain_name in domains_to_load:
            domain_dir = self.base_dir / "domains" / domain_name
            if domain_dir.exists():
                self._load_tasks_from_directory(domain_dir, None, domain_name)

    def _load_tasks_from_directory(
        self, directory: Path, level: Optional[str] = None, domain: Optional[str] = None
    ):
        """ä»ç›®å½•åŠ è½½ä»»åŠ¡"""
        for task_file in directory.glob("*.yaml"):
            try:
                with open(task_file, "r", encoding="utf-8") as f:
                    task_data = yaml.safe_load(f)

                task = TestTask(
                    id=task_data["id"],
                    level=level or task_data.get("level", "unknown"),
                    domain=domain or task_data.get("domain", "unknown"),
                    title=task_data["title"],
                    description=task_data["description"],
                    input_spec=task_data["input_spec"],
                    output_spec=task_data["output_spec"],
                    evaluation_criteria=task_data["evaluation_criteria"],
                    test_cases=task_data["test_cases"],
                    time_limit=task_data.get("time_limit", 300),
                )

                self.test_tasks[task.id] = task
                logger.info(f"åŠ è½½ä»»åŠ¡: {task.id} - {task.title}")

            except Exception as e:
                logger.error(f"åŠ è½½ä»»åŠ¡æ–‡ä»¶ {task_file} å¤±è´¥: {e}")

    async def run_single_task(self, task: TestTask) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ä»»åŠ¡"""
        logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task.id} - {task.title}")
        start_time = time.time()

        try:
            # åˆ›å»ºä»»åŠ¡ç‰¹å®šçš„æ²™ç®±ç¯å¢ƒ
            task_sandbox = self.sandbox_dir / f"task_{task.id}_{int(start_time)}"
            task_sandbox.mkdir(exist_ok=True)

            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„AI Agentè¿›è¡Œä»£ç ç”Ÿæˆ
            # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿçš„ä»£ç ç”Ÿæˆ
            generated_code = await self._simulate_code_generation(task)

            # æ‰§è¡ŒåŠŸèƒ½æµ‹è¯•
            test_results = await self._run_functional_tests(
                task, generated_code, task_sandbox
            )

            # è¯„ä¼°ä»£ç è´¨é‡
            quality_score = self._evaluate_code_quality(task, generated_code)

            # è®¡ç®—æ€»åˆ†
            total_score, max_score, details = self._calculate_score(
                task, test_results, quality_score
            )

            execution_time = time.time() - start_time

            result = TestResult(
                task_id=task.id,
                success=True,
                score=total_score,
                max_score=max_score,
                execution_time=execution_time,
                details=details,
                generated_code=generated_code,
            )

            logger.info(f"ä»»åŠ¡å®Œæˆ: {task.id}, å¾—åˆ†: {total_score:.2f}/{max_score}")
            return result

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task.id}, é”™è¯¯: {e}")

            return TestResult(
                task_id=task.id,
                success=False,
                score=0.0,
                max_score=100.0,
                execution_time=execution_time,
                details={"error": str(e)},
                error_message=str(e),
            )

    async def _simulate_code_generation(self, task: TestTask) -> str:
        """ä»£ç ç”Ÿæˆ - ä¼˜å…ˆä½¿ç”¨RAG Agentï¼Œå¤‡ç”¨ä¼ ç»Ÿæ¨¡æ¿"""
        # å¦‚æœRAG Agentå¯ç”¨ï¼Œä½¿ç”¨RAG Agentç”Ÿæˆä»£ç 
        if self.rag_agent:
            try:
                logger.info(f"ğŸ¤– ä½¿ç”¨RAG Code Agentç”Ÿæˆä»£ç : {task.title}")

                # æ„å»ºä»»åŠ¡æè¿°
                rag_task_description = f"""
                è¯·æ ¹æ®ä»¥ä¸‹ä»»åŠ¡è¦æ±‚ç”Ÿæˆä»£ç :
                
                ä»»åŠ¡æ ‡é¢˜: {task.title}
                ä»»åŠ¡æè¿°: {task.description}
                çº§åˆ«: {task.level}
                é¢†åŸŸ: {task.domain}
                
                è¾“å…¥è§„æ ¼: {task.input_spec}
                è¾“å‡ºè§„æ ¼: {task.output_spec}
                è¯„ä¼°æ ‡å‡†: {task.evaluation_criteria}
                
                è¯·ç”Ÿæˆç¬¦åˆè¦æ±‚çš„å®Œæ•´ä»£ç å®ç°ã€‚
                """

                # ä½¿ç”¨RAG Agentæ‰§è¡Œä»»åŠ¡
                result = await self.rag_agent.execute_task(
                    task_description=rag_task_description, max_iterations=3
                )

                if result.get("success") and result.get("results"):
                    # æå–ç”Ÿæˆçš„ä»£ç 
                    for step_result in result["results"]:
                        if step_result.get("generated_code"):
                            logger.info("âœ… RAG Agent æˆåŠŸç”Ÿæˆä»£ç ")
                            return step_result["generated_code"]
                        elif (
                            step_result.get("output") and "```" in step_result["output"]
                        ):
                            # å°è¯•ä»è¾“å‡ºä¸­æå–ä»£ç å—
                            output = step_result["output"]
                            code_start = output.find("```python")
                            if code_start == -1:
                                code_start = output.find("```")
                            if code_start != -1:
                                code_start = output.find("\n", code_start) + 1
                                code_end = output.find("```", code_start)
                                if code_end != -1:
                                    extracted_code = output[code_start:code_end].strip()
                                    logger.info("âœ… ä»RAG Agentè¾“å‡ºä¸­æå–ä»£ç ")
                                    return extracted_code

                logger.warning("âš ï¸ RAG Agent æœªç”Ÿæˆæœ‰æ•ˆä»£ç ï¼Œä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ")

            except Exception as e:
                logger.error(f"âŒ RAG Agent ä»£ç ç”Ÿæˆå¤±è´¥: {e}")

        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ä¼ ç»Ÿæ¨¡æ¿ç”Ÿæˆ
        logger.info(f"ğŸ“ ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆä»£ç : {task.title}")
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿç”Ÿæˆæ—¶é—´

        # æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›ä¸åŒçš„ç¤ºä¾‹ä»£ç å¹¶ä¿å­˜åˆ°æ–‡ä»¶
        generated_code = ""
        file_extension = ""

        if task.domain == "algorithms":
            generated_code = """
def temperature_converter(value, unit):
    if unit == 'C':
        return round((value * 9/5) + 32, 1), 'F'
    elif unit == 'F':
        return round((value - 32) * 5/9, 1), 'C'
    else:
        raise ValueError("Invalid unit")
"""
            file_extension = ".py"
        elif task.domain == "web_development":
            generated_code = """
<!DOCTYPE html>
<html>
<head>
    <title>Personal Resume</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .section { margin-bottom: 20px; }
        h1, h2 { color: #333; }
    </style>
</head>
<body>
    <h1 id="name">John Doe</h1>
    <div class="section">
        <h2>Contact</h2>
        <p id="email">john@example.com</p>
        <p id="phone">123-456-7890</p>
    </div>
    <div class="section">
        <h2>Education</h2>
        <p id="education">Computer Science Degree</p>
    </div>
</body>
</html>
"""
            file_extension = ".html"
        else:
            generated_code = "# ç¤ºä¾‹ä»£ç \nprint('Hello, World!')"
            file_extension = ".py"

        # å°†ç”Ÿæˆçš„ä»£ç ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        try:
            code_file = (
                self.generated_code_dir
                / f"{task.id}_{task.level}_{task.domain}{file_extension}"
            )
            with open(code_file, "w", encoding="utf-8") as f:
                f.write(generated_code)
            logger.info(f"ğŸ’¾ ä»£ç å·²ä¿å­˜åˆ°: {code_file}")
        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜ä»£ç æ–‡ä»¶å¤±è´¥: {e}")

        return generated_code

    async def _run_functional_tests(
        self, task: TestTask, code: str, sandbox_dir: Path
    ) -> Dict[str, Any]:
        """è¿è¡ŒåŠŸèƒ½æµ‹è¯•"""
        test_results = {
            "passed": 0,
            "failed": 0,
            "total": len(task.test_cases),
            "details": [],
        }

        for i, test_case in enumerate(task.test_cases):
            try:
                # è¿™é‡Œåº”è¯¥å®é™…æ‰§è¡Œä»£ç å’Œæµ‹è¯•ç”¨ä¾‹
                # æš‚æ—¶æ¨¡æ‹Ÿæµ‹è¯•ç»“æœ
                passed = i % 3 != 0  # æ¨¡æ‹Ÿéƒ¨åˆ†æµ‹è¯•é€šè¿‡

                test_results["details"].append(
                    {
                        "test_case": i,
                        "passed": passed,
                        "input": test_case.get("input", {}),
                        "expected": test_case.get("expected", {}),
                        "actual": "æ¨¡æ‹Ÿè¾“å‡º",
                    }
                )

                if passed:
                    test_results["passed"] += 1
                else:
                    test_results["failed"] += 1

            except Exception as e:
                test_results["failed"] += 1
                test_results["details"].append(
                    {"test_case": i, "passed": False, "error": str(e)}
                )

        return test_results

    def _evaluate_code_quality(self, task: TestTask, code: str) -> Dict[str, float]:
        """è¯„ä¼°ä»£ç è´¨é‡"""
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„ä»£ç è´¨é‡è¯„ä¼°
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿåˆ†æ•°
        return {
            "syntax": 85.0,
            "structure": 80.0,
            "readability": 75.0,
            "best_practices": 70.0,
        }

    def _calculate_score(
        self,
        task: TestTask,
        test_results: Dict[str, Any],
        quality_scores: Dict[str, float],
    ) -> tuple[float, float, Dict[str, Any]]:
        """è®¡ç®—ä»»åŠ¡æ€»åˆ†"""
        criteria = self.config["evaluation"].get(
            task.level, self.config["evaluation"]["beginner"]
        )

        # åŠŸèƒ½æ€§å¾—åˆ†
        functionality_score = (
            (test_results["passed"] / test_results["total"]) * 100
            if test_results["total"] > 0
            else 0
        )

        # ä»£ç è´¨é‡å¾—åˆ†
        quality_score = sum(quality_scores.values()) / len(quality_scores)

        # åŠ æƒè®¡ç®—æ€»åˆ†
        total_score = 0
        max_score = 100
        details = {}

        # æ ¹æ®è¯„ä¼°æ ‡å‡†è®¡ç®—åˆ†æ•°
        for criterion, weight in criteria.items():
            if criterion == "functionality":
                score = functionality_score * (weight / 100)
            else:
                score = quality_score * (weight / 100)

            total_score += score
            details[criterion] = {"score": score, "weight": weight, "max_score": weight}

        details["test_results"] = test_results
        details["quality_scores"] = quality_scores

        return total_score, max_score, details

    async def run_tests(
        self, level: Optional[str] = None, domain: Optional[str] = None
    ) -> List[TestResult]:
        """è¿è¡Œæµ‹è¯•"""
        logger.info("å¼€å§‹æ‰§è¡ŒBenchmarkæµ‹è¯•")

        # åŠ è½½æµ‹è¯•ä»»åŠ¡
        self.load_test_tasks(level, domain)

        if not self.test_tasks:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°è¦æ‰§è¡Œçš„æµ‹è¯•ä»»åŠ¡")
            return []

        logger.info(f"æ‰¾åˆ° {len(self.test_tasks)} ä¸ªæµ‹è¯•ä»»åŠ¡")

        # å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
        semaphore = asyncio.Semaphore(self.config.get("parallel_tasks", 3))

        async def run_with_semaphore(task):
            async with semaphore:
                return await self.run_single_task(task)

        tasks = [run_with_semaphore(task) for task in self.test_tasks.values()]
        self.test_results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸ç»“æœ
        valid_results = []
        for result in self.test_results:
            if isinstance(result, Exception):
                logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {result}")
            else:
                valid_results.append(result)

        self.test_results = valid_results
        logger.info(f"æµ‹è¯•å®Œæˆï¼Œå…±æ‰§è¡Œ {len(self.test_results)} ä¸ªä»»åŠ¡")

        return self.test_results

    def generate_report(self, output_format: str = "json") -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.test_results:
            logger.warning("æ²¡æœ‰æµ‹è¯•ç»“æœå¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return ""

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        if output_format == "json":
            return self._generate_json_report(timestamp)
        elif output_format == "html":
            return self._generate_html_report(timestamp)
        else:
            logger.error(f"ä¸æ”¯æŒçš„æŠ¥å‘Šæ ¼å¼: {output_format}")
            return ""

    def _generate_json_report(self, timestamp: str) -> str:
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        report_data = {
            "timestamp": timestamp,
            "summary": self._get_summary_stats(),
            "results": [asdict(result) for result in self.test_results],
            "config": self.config,
        }

        report_file = self.reports_dir / f"benchmark_report_{timestamp}.json"

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        logger.info(f"JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)

    def _generate_html_report(self, timestamp: str) -> str:
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š"""
        # è¿™é‡Œåº”è¯¥ç”Ÿæˆè¯¦ç»†çš„HTMLæŠ¥å‘Š
        # æš‚æ—¶ç”Ÿæˆç®€å•çš„HTML
        summary = self._get_summary_stats()

        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>AI Agent Benchmark Report - {timestamp}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .summary {{ background: #f5f5f5; padding: 20px; margin-bottom: 20px; }}
        .result {{ border: 1px solid #ddd; margin: 10px 0; padding: 15px; }}
        .success {{ border-left: 5px solid #4CAF50; }}
        .failure {{ border-left: 5px solid #f44336; }}
    </style>
</head>
<body>
    <h1>AI Agent Benchmark Test Report</h1>
    <div class="summary">
        <h2>æµ‹è¯•æ€»ç»“</h2>
        <p>æ€»ä»»åŠ¡æ•°: {summary['total_tasks']}</p>
        <p>æˆåŠŸä»»åŠ¡: {summary['successful_tasks']}</p>
        <p>å¤±è´¥ä»»åŠ¡: {summary['failed_tasks']}</p>
        <p>å¹³å‡åˆ†æ•°: {summary['average_score']:.2f}</p>
        <p>æ€»æ‰§è¡Œæ—¶é—´: {summary['total_time']:.2f}ç§’</p>
    </div>
    
    <h2>è¯¦ç»†ç»“æœ</h2>
"""

        for result in self.test_results:
            status_class = "success" if result.success else "failure"
            html_content += f"""
    <div class="result {status_class}">
        <h3>ä»»åŠ¡: {result.task_id}</h3>
        <p>çŠ¶æ€: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}</p>
        <p>åˆ†æ•°: {result.score:.2f}/{result.max_score}</p>
        <p>æ‰§è¡Œæ—¶é—´: {result.execution_time:.2f}ç§’</p>
        {f'<p>é”™è¯¯: {result.error_message}</p>' if result.error_message else ''}
    </div>
"""

        html_content += """
</body>
</html>
"""

        report_file = self.reports_dir / f"benchmark_report_{timestamp}.html"

        with open(report_file, "w", encoding="utf-8") as f:
            f.write(html_content)

        logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)

    def _get_summary_stats(self) -> Dict[str, Any]:
        """è·å–æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        if not self.test_results:
            return {}

        successful_tasks = sum(1 for r in self.test_results if r.success)
        total_score = sum(r.score for r in self.test_results)
        total_max_score = sum(r.max_score for r in self.test_results)
        total_time = sum(r.execution_time for r in self.test_results)

        return {
            "total_tasks": len(self.test_results),
            "successful_tasks": successful_tasks,
            "failed_tasks": len(self.test_results) - successful_tasks,
            "average_score": (
                total_score / len(self.test_results) if self.test_results else 0
            ),
            "total_score": total_score,
            "total_max_score": total_max_score,
            "total_time": total_time,
            "success_rate": (
                successful_tasks / len(self.test_results) * 100
                if self.test_results
                else 0
            ),
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AI Agentç¼–ç¨‹èƒ½åŠ›æµ‹è¯•é›†åˆ")
    parser.add_argument(
        "--level",
        choices=[
            "beginner",
            "elementary",
            "intermediate",
            "advanced",
            "expert",
            "master",
            "all",
        ],
        default="all",
        help="æŒ‡å®šæµ‹è¯•éš¾åº¦çº§åˆ«",
    )
    parser.add_argument(
        "--domain",
        choices=[
            "web_development",
            "mobile_app",
            "algorithms",
            "devops",
            "data_science",
            "all",
        ],
        default="all",
        help="æŒ‡å®šæµ‹è¯•æŠ€æœ¯é¢†åŸŸ",
    )
    parser.add_argument(
        "--output", choices=["json", "html"], default="json", help="è¾“å‡ºæŠ¥å‘Šæ ¼å¼"
    )
    parser.add_argument(
        "--config", default="config/test_config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--workspace", type=str, help="æŒ‡å®šå·¥ä½œåŒºè·¯å¾„ï¼ˆç”¨äºRAG Code Agentæµ‹è¯•ï¼‰"
    )
    parser.add_argument(
        "--enable-rag", action="store_true", help="å¯ç”¨RAG Code Agentæµ‹è¯•åŠŸèƒ½"
    )

    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = BenchmarkTestRunner(args.config, workspace_path=args.workspace)

    try:
        # è¿è¡Œæµ‹è¯•
        results = asyncio.run(runner.run_tests(args.level, args.domain))

        # ç”ŸæˆæŠ¥å‘Š
        report_file = runner.generate_report(args.output)

        # æ‰“å°æ‘˜è¦
        summary = runner._get_summary_stats()
        print(f"\n{'='*50}")
        print("æµ‹è¯•å®Œæˆï¼")
        print(f"æ€»ä»»åŠ¡æ•°: {summary.get('total_tasks', 0)}")
        print(f"æˆåŠŸä»»åŠ¡: {summary.get('successful_tasks', 0)}")
        print(f"æˆåŠŸç‡: {summary.get('success_rate', 0):.1f}%")
        print(f"å¹³å‡åˆ†æ•°: {summary.get('average_score', 0):.2f}")
        print(f"æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        print(f"{'='*50}")

    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
