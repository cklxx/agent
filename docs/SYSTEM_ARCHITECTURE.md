# System Architecture Document

## Introduction
This document provides an overview of the system architecture for the DeepTool project. It outlines the core components, their interactions, and the overall design philosophy.

## Core Philosophy
The DeepTool project is built upon a LangGraph-based multi-agent system. This architecture allows for modular, maintainable, and scalable development of complex AI-driven workflows.

## Entry Points
The system has the following primary entry points:
*   `main.py`: Handles Command Line Interface (CLI) interactions and invokes the main agent workflow via `src.workflow.run_agent_workflow_async`.
*   `server.py`: Hosts the FastAPI web server, providing a User Interface (UI) and API endpoints for interacting with the system.
*   `src.workflow.py`: Orchestrates the main LangGraph execution using `graph.astream`, managing the flow of tasks between different agents.

## Main Workflow (`src.workflow.py` and `src.graph.builder.py`)
The main workflow is defined and executed using LangGraph.
*   `build_graph()` (in `src.graph.builder.py`): This function is responsible for constructing the LangGraph, defining the nodes (agents) and edges (transitions) that represent the workflow logic.
*   Primary Nodes (defined in `src.graph.nodes.py`):
    *   `Coordinator`: Manages the overall task, breaking it down and delegating to other agents.
    *   `Planner`: Develops detailed plans for complex tasks.
    *   `BackgroundInvestigator`: Performs research and information gathering in the background.
    *   `HumanFeedback`: Allows for human intervention and guidance within the workflow.
    *   `ResearchTeam`: A sub-graph or group of agents focused on research tasks.
    *   `Researcher`: An individual agent responsible for specific research queries.
    *   `Coder`: An agent specialized in writing and modifying code.
    *   `Reporter`: Compiles results and generates final reports or outputs.

## Agent Implementations (`src.agents/`)
Specialized agents are implemented in the `src.agents/` directory.
*   `code_agent.py`:
    *   `CodeTaskPlanner`: An LLM-based planner (with fallback mechanisms) that devises multi-phase plans for coding tasks, typically including pre-analysis, implementation, and verification phases.
    *   `create_code_agent`: A factory function that creates a REACT (Reasoning and Acting) agent specifically tailored for code generation and modification tasks.
*   `rag_enhanced_code_agent.py`:
    *   `RAGEnhancedCodeTaskPlanner`: Extends the `CodeTaskPlanner` by incorporating Retrieval Augmented Generation (RAG). This allows the planner to use context from the existing codebase (retrieved via code indexing and searching) and project structure analysis to create more informed plans.
    *   `RAGEnhancedCodeAgent`: Wraps the REACT agent created by `create_code_agent` and injects the RAG-retrieved context into the prompts, enabling the agent to generate code that is more consistent with the existing project.
*   Workflows (`src.code_agent_workflow.py`, `src.rag_enhanced_code_agent_workflow.py`): These modules orchestrate the execution of the specialized code agents, managing their lifecycle and interactions.

## RAG Subsystem (`src.rag/`, `src.context/code_rag_adapter.py`)
The Retrieval Augmented Generation (RAG) subsystem provides context from the codebase to the agents.
*   `CodeIndexer` (`src.rag/code_indexer.py`): Scans the repository, parses code files (using Abstract Syntax Trees - ASTs for Python), chunks the code, and stores embeddings and metadata in an SQLite database for efficient retrieval.
*   `CodeRetriever` (`src.rag/code_retriever.py`): Queries the SQLite index to find code snippets relevant to a given query or task.
*   `CodeRAGAdapter` (`src.context/code_rag_adapter.py`): Integrates the RAG functionalities (indexing and retrieval) with the `ContextManager`, making codebase context available to agents.
*   For more detailed information on the RAG subsystem, refer to `docs/CODE_RAG_README.md`.

## Tool System (`src.tools/`)
The system includes a comprehensive tool system that agents can use to interact with the environment.
*   Overview: Tools include capabilities for web searching, file operations (read, write, list), executing terminal commands, running Python code in a REPL, crawling websites, interacting with maps, and text-to-speech (TTS).
*   MCP Integration: The tool system is integrated with the Multi-Context Prompting (MCP) framework, allowing for extensible and versioned toolsets.
*   A detailed list of available tools and their usage can be found in `docs/agent.md`.

## Configuration (`.env`, `conf.yaml`, `src.config/`)
System configuration is managed through several files:
*   `.env`: Stores environment-specific settings, such as API keys for external services and local development flags.
*   `conf.yaml`: Contains configuration for LLM models (e.g., model names, API endpoints), service URLs, and other semi-static parameters.
*   `src.config/configuration.py`: This module loads, validates, and provides application-wide access to the settings from `.env` and `conf.yaml`.

## LLM Integration (`src.llms/`)
The system integrates with Large Language Models (LLMs) through an abstraction layer.
*   Abstraction: Utilizes LiteLLM to provide a consistent interface for interacting with various LLM providers (e.g., OpenAI, Anthropic, local models).
*   Categorization: LLMs are categorized based on their intended use (e.g., 'reasoning' for complex tasks, 'basic' for simpler tasks), allowing the system to select appropriate models dynamically.

## Web Interface (`web/`)
A web-based user interface is provided for interacting with the DeepTool system.
*   Technology: Developed as a Next.js application.
*   Communication: Interacts with the Python backend through API calls defined in `server.py`.

## Data Flow Summary
A typical user request flows through the system as follows:
1.  User initiates a task via the CLI (`main.py`) or the Web UI (`server.py`).
2.  The request is routed to the `Coordinator` node in the main LangGraph workflow (`src.workflow.py`).
3.  The `Coordinator` may delegate to the `Planner` to break down the task into smaller steps.
4.  Depending on the task, specialized agents like `Coder` (for code tasks) or `ResearchTeam` (for information gathering) are invoked.
    *   These agents may use tools from `src.tools/` and context from the RAG subsystem (`src.rag/`).
5.  Human feedback may be solicited via the `HumanFeedback` node if required.
6.  Finally, the `Reporter` node compiles the results and generates the final output, which is returned to the user.

## Automated Codebase Analysis Summary
*   Total Files: 340
*   Main Languages: css (4 files), javascript (5 files), json (7 files), markdown (40 files), python (114 files)
*   Total Chunks (from RAG indexing): 9419
*   Common Code Patterns/Themes:
    *   {'file_path': 'src/rag_enhanced_code_agent_workflow.py', 'title': 'rag_enhanced_code_agent_workflow.py (python)', 'url': 'file://src/rag_enhanced_code_agent_workflow.py', 'chunks': [{'content': 'class RAGEnhancedCodeAgentWorkflow:\n    """RAGå¢å¼ºçš„ä»£ç ä»£ç†å·¥ä½œæµ"""\n\n    def __init__(self, repo_path: str = "."):\n        """åˆå§‹åŒ–RAGå¢å¼ºä»£ç ä»£ç†å·¥ä½œæµ"""\n        logger.info("åˆå§‹åŒ–RAGå¢å¼ºä»£ç ä»£ç†å·¥ä½œæµ")\n\n        self.repo_path = repo_path\n\n        # å®šä¹‰å¯ç”¨çš„å·¥å…·\n        self.code_tools = [\n            # å‘½ä»¤è¡Œå·¥å…·\n            execute_terminal_command,\n            get_current_directory,\n            list_directory_contents,\n            # æ–‡ä»¶è¯»å–å·¥å…·\n            read_file,\n            read_file_lines,\n            get_file_info,\n            # æ–‡ä»¶å†™å…¥å·¥å…·\n            write_file,\n            append_to_file,\n            create_new_file,\n            generate_file_diff,\n        ]\n\n        logger.info(f"é…ç½® {len(self.code_tools)} ä¸ªå·¥å…·")\n\n        # åˆ›å»ºRAGå¢å¼ºçš„code agent\n        try:\n            self.agent = create_rag_enhanced_code_agent(\n                repo_path=repo_path, tools=self.code_tools\n            )\n            logger.info("RAGå¢å¼ºä»£ç ä»£ç†åˆ›å»ºæˆåŠŸ")\n        except Exception as e:\n            logger.error(f"åˆ›å»ºRAGå¢å¼ºä»£ç ä»£ç†å¤±è´¥: {str(e)}")\n            raise\n\n    async def execute_task(\n        self, task_description: str, max_iterations: int = 5\n    ) -> Dict[str, Any]:\n        """\n        æ‰§è¡ŒRAGå¢å¼ºçš„ä»£ç ä»»åŠ¡\n\n        Args:\n            task_description: ä»»åŠ¡æè¿°\n            max_iterations: æœ€å¤§æ‰§è¡Œè½®æ¬¡\n\n        Returns:\n            ä»»åŠ¡æ‰§è¡Œç»“æœ\n        """\n        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡ŒRAGå¢å¼ºä»£ç ä»»åŠ¡")\n        logger.info(\n            f"ğŸ“‹ ä»»åŠ¡æè¿°: {task_description[:100]}{\'...\' if len(task_description) > 100 else \'\'}"\n        )\n\n        try:\n            # ä½¿ç”¨RAGå¢å¼ºagentæ‰§è¡Œä»»åŠ¡\n            result = await self.agent.execute_task_with_rag(\n                task_description=task_description, max_iterations=max_iterations\n            )\n\n            # å¢å¼ºç»“æœä¿¡æ¯\n            enhanced_result = self._enhance_result(result, task_description)\n\n            # è®°å½•æ‰§è¡Œç»“æœ\n            if enhanced_result.get("success"):\n                logger.info("ğŸ‰ RAGå¢å¼ºä»»åŠ¡æ‰§è¡ŒæˆåŠŸ!")\n            else:\n                logger.warning("âš ï¸ RAGå¢å¼ºä»»åŠ¡æ‰§è¡Œéƒ¨åˆ†æˆåŠŸæˆ–å¤±è´¥")\n\n            success_count = enhanced_result.get("successful_steps", 0)\n            total_steps = enhanced_result.get("total_steps", 0)\n            relevant_files = enhanced_result.get("relevant_files_analyzed", 0)\n\n            logger.info(f"ğŸ“ˆ æ‰§è¡Œç»Ÿè®¡: {success_count}/{total_steps} æ­¥éª¤æˆåŠŸ")\n            logger.info(f"ğŸ” RAGåˆ†æ: {relevant_files} ä¸ªç›¸å…³æ–‡ä»¶")\n\n            return enhanced_result\n\n        except Exception as e:\n            logger.error(f"âŒ RAGå¢å¼ºä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")\n            return {\n                "success": False,\n                "error": str(e),\n                "rag_enhanced": True,\n                "workflow_type": "rag_enhanced",\n            }\n\n    def _enhance_result(\n        self, result: Dict[str, Any], task_description: str\n    ) -> Dict[str, Any]:\n        """å¢å¼ºæ‰§è¡Œç»“æœä¿¡æ¯"""\n        enhanced = {\n            **result,\n            "workflow_type": "rag_enhanced",\n            "repo_path": self.repo_path,\n            "task_description": task_description,\n            "enhancement_features": [\n                "RAGä»£ç æ£€ç´¢",\n                "ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§„åˆ’",\n                "æ¨¡å¼ä¸€è‡´æ€§éªŒè¯",\n                "æ™ºèƒ½ä»£ç ç”Ÿæˆ",\n            ],\n        }\n\n        # æ·»åŠ è´¨é‡æŒ‡æ ‡\n        if result.get("results"):\n            rag_enhanced_steps = sum(\n                1 for r in result["results"] if r.get("rag_enhanced", False)\n            )\n            enhanced["rag_enhancement_rate"] = (\n                rag_enhanced_steps / len(result["results"]) if result["results"] else 0\n            )\n\n        return enhanced\n\n    async def analyze_codebase(self) -> Dict[str, Any]:\n        """åˆ†æä»£ç åº“ç»“æ„å’Œæ¨¡å¼"""\n        logger.info("ğŸ” å¼€å§‹åˆ†æä»£ç åº“...")\n\n        try:\n            # ä½¿ç”¨agentçš„å†…éƒ¨ç»„ä»¶è¿›è¡Œåˆ†æ\n            task_planner = self.agent.task_planner\n\n            # åˆ†æé¡¹ç›®ç»“æ„\n            project_info = await task_planner._analyze_project_structure()\n\n            # æ£€ç´¢å…³é”®ä»£ç æ¨¡å¼ï¼ˆä½¿ç”¨é€šç”¨æŸ¥è¯¢ï¼‰\n            common_patterns = await task_planner._retrieve_relevant_code(\n                "class function implementation pattern"\n            )\n\n            analysis_result = {\n                "project_structure": project_info,\n                "common_patterns": common_patterns,\n                "analysis_timestamp": asyncio.get_event_loop().time(),\n                "repo_path": self.repo_path,\n            }\n\n            logger.info(f"âœ… ä»£ç åº“åˆ†æå®Œæˆ")\n            logger.info(f"   ğŸ“ æ€»æ–‡ä»¶æ•°: {project_info.get(\'total_files\', 0)}")\n            logger.info(\n                f"   ğŸ”¤ ä¸»è¦è¯­è¨€: {\', \'.join(project_info.get(\'main_languages\', [])[:3])}"\n            )\n            logger.info(f"   ğŸ“‹ å‘ç°æ¨¡å¼: {len(common_patterns)} ä¸ª")\n\n            return analysis_result\n\n        except Exception as e:\n            logger.error(f"âŒ ä»£ç åº“åˆ†æå¤±è´¥: {str(e)}")\n            return {"success": False, "error": str(e)}\n\n    async def suggest_improvements(self, focus_area: str = "") -> Dict[str, Any]:\n        """åŸºäºRAGåˆ†æå»ºè®®ä»£ç æ”¹è¿›"""\n        logger.info("ğŸ’¡ ç”Ÿæˆä»£ç æ”¹è¿›å»ºè®®...")\n\n        try:\n            # æ„å»ºæ”¹è¿›å»ºè®®ä»»åŠ¡\n            improvement_task = f"""\n            åŸºäºå½“å‰ä»£ç åº“çš„åˆ†æï¼Œè¯·æä¾›ä»£ç æ”¹è¿›å»ºè®®ã€‚\n            {\'é‡ç‚¹å…³æ³¨: \' + focus_area if focus_area else \'\'}\n            \n            è¯·åˆ†æ:\n            1. ä»£ç ç»“æ„å’Œæ¶æ„æ”¹è¿›æœºä¼š\n            2. æ€§èƒ½ä¼˜åŒ–å»ºè®®\n            3. å¯ç»´æŠ¤æ€§æå‡æ–¹æ¡ˆ\n            4. å®‰å…¨æ€§å¢å¼ºå»ºè®®\n            5. æµ‹è¯•è¦†ç›–ç‡æ”¹è¿›\n            """\n\n            # æ‰§è¡Œåˆ†æä»»åŠ¡\n            result = await self.execute_task(improvement_task)\n\n            if result.get("success"):\n                logger.info("âœ… æ”¹è¿›å»ºè®®ç”ŸæˆæˆåŠŸ")\n            else:\n                logger.warning("âš ï¸ æ”¹è¿›å»ºè®®ç”Ÿæˆéƒ¨åˆ†æˆåŠŸ")\n\n            return result\n\n        except Exception as e:\n            logger.error(f"âŒ æ”¹è¿›å»ºè®®ç”Ÿæˆå¤±è´¥: {str(e)}")\n            return {"success": False, "error": str(e)}\n\n    async def execute_refactoring(\n        self, target_files: List[str], refactoring_goals: str\n    ) -> Dict[str, Any]:\n        """æ‰§è¡Œä»£ç é‡æ„ä»»åŠ¡"""\n        logger.info(f"ğŸ”„ å¼€å§‹ä»£ç é‡æ„...")\n        logger.info(\n            f"ğŸ“ ç›®æ ‡æ–‡ä»¶: {\', \'.join(target_files[:3])}{\'...\' if len(target_files) > 3 else \'\'}"\n        )\n\n        try:\n            # æ„å»ºé‡æ„ä»»åŠ¡\n            refactoring_task = f"""\n            è¯·å¯¹ä»¥ä¸‹æ–‡ä»¶è¿›è¡Œé‡æ„:\n            {chr(10).join(f"- {file}" for file in target_files)}\n            \n            é‡æ„ç›®æ ‡:\n            {refactoring_goals}\n            \n            è¦æ±‚:\n            1. ä¿æŒç°æœ‰åŠŸèƒ½å®Œæ•´æ€§\n            2. éµå¾ªé¡¹ç›®ä»£ç é£æ ¼å’Œæ¨¡å¼\n            3. æé«˜ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§\n            4. ç¡®ä¿å‘åå…¼å®¹æ€§\n            5. æ·»åŠ å¿…è¦çš„æµ‹è¯•éªŒè¯\n            """\n\n            # æ‰§è¡Œé‡æ„ä»»åŠ¡\n            result = await self.execute_task(refactoring_task)\n\n            # å¢å¼ºé‡æ„ç»“æœ\n            if result.get("success"):\n                result["refactoring_files"] = target_files\n                result["refactoring_goals"] = refactoring_goals\n                logger.info("âœ… ä»£ç é‡æ„å®Œæˆ")\n            else:\n                logger.warning("âš ï¸ ä»£ç é‡æ„éƒ¨åˆ†å®Œæˆ")\n\n            return result\n\n        except Exception as e:\n            logger.error(f"âŒ ä»£ç é‡æ„å¤±è´¥: {str(e)}")\n            return {\n                "success": False,\n                "error": str(e),\n                "refactoring_files": target_files,\n            }\n\n    async def generate_documentation(self, doc_type: str = "api") -> Dict[str, Any]:\n        """ç”Ÿæˆé¡¹ç›®æ–‡æ¡£"""\n        logger.info(f"ğŸ“š å¼€å§‹ç”Ÿæˆ {doc_type} æ–‡æ¡£...")\n\n        try:\n            # æ„å»ºæ–‡æ¡£ç”Ÿæˆä»»åŠ¡\n            if doc_type.lower() == "api":\n                doc_task = """\n                åŸºäºå½“å‰ä»£ç åº“ç”ŸæˆAPIæ–‡æ¡£:\n                1. åˆ†ææ‰€æœ‰å…¬å¼€çš„ç±»å’Œå‡½æ•°\n                2. æå–æ–‡æ¡£å­—ç¬¦ä¸²å’Œæ³¨é‡Š\n                3. ç”Ÿæˆç»“æ„åŒ–çš„APIæ–‡æ¡£\n                4. åŒ…å«ä½¿ç”¨ç¤ºä¾‹å’Œå‚æ•°è¯´æ˜\n                5. éµå¾ªé¡¹ç›®æ–‡æ¡£é£æ ¼\n                """\n            elif doc_type.lower() == "architecture":\n                doc_task = """\n                ç”Ÿæˆé¡¹ç›®æ¶æ„æ–‡æ¡£:\n                1. åˆ†æé¡¹ç›®æ•´ä½“ç»“æ„\n                2. è¯†åˆ«æ ¸å¿ƒç»„ä»¶å’Œä¾èµ–å…³ç³»\n                3. ç”Ÿæˆæ¶æ„å›¾å’Œè¯´æ˜\n                4. æè¿°è®¾è®¡æ¨¡å¼å’Œæœ€ä½³å®è·µ\n                5. åŒ…å«éƒ¨ç½²å’Œé…ç½®è¯´æ˜\n                """\n            else:\n                doc_task = f"""\n                ç”Ÿæˆ {doc_type} ç›¸å…³æ–‡æ¡£:\n                1. åˆ†æç›¸å…³ä»£ç å’Œç»„ä»¶\n                2. æå–å…³é”®ä¿¡æ¯å’Œæ¨¡å¼\n                3. ç”Ÿæˆç»“æ„åŒ–æ–‡æ¡£\n                4. åŒ…å«ç¤ºä¾‹å’Œæœ€ä½³å®è·µ\n                """\n\n            # æ‰§è¡Œæ–‡æ¡£ç”Ÿæˆä»»åŠ¡\n            result = await self.execute_task(doc_task)\n\n            if result.get("success"):\n                result["documentation_type"] = doc_type\n                logger.info(f"âœ… {doc_type} æ–‡æ¡£ç”Ÿæˆå®Œæˆ")\n            else:\n                logger.warning(f"âš ï¸ {doc_type} æ–‡æ¡£ç”Ÿæˆéƒ¨åˆ†å®Œæˆ")\n\n            return result\n\n        except Exception as e:\n            logger.error(f"âŒ æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {str(e)}")\n            return {"success": False, "error": str(e), "documentation_type": doc_type}\n\n    def get_available_tools(self) -> List[str]:\n        """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""\n        return [getattr(tool, "name", str(tool)) for tool in self.code_tools]\n\n    def get_workflow_capabilities(self) -> Dict[str, Any]:\n        """è·å–å·¥ä½œæµèƒ½åŠ›æè¿°"""\n        return {\n            "core_features": [\n                "RAGå¢å¼ºä»£ç ç”Ÿæˆ",\n                "ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä»»åŠ¡è§„åˆ’",\n                "æ¨¡å¼ä¸€è‡´æ€§ä¿è¯",\n                "æ™ºèƒ½ä»£ç é‡æ„",\n                "è‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆ",\n            ],\n            "supported_tasks": [\n                "æ–°åŠŸèƒ½å¼€å‘",\n                "ä»£ç é‡æ„",\n                "Bugä¿®å¤",\n                "æ€§èƒ½ä¼˜åŒ–",\n                "æ–‡æ¡£ç”Ÿæˆ",\n                "æ¶æ„åˆ†æ",\n            ],\n            "quality_metrics": ["æ¨¡å¼ä¸€è‡´æ€§", "ä»£ç é‡ç”¨ç‡", "é›†æˆè´¨é‡", "çº¦å®šéµå¾ªç‡"],\n            "tools_count": len(self.code_tools),\n            "repo_path": self.repo_path,\n        }', 'similarity': 0.8}, {'content': '    async def analyze_codebase(self) -> Dict[str, Any]:\n        """åˆ†æä»£ç åº“ç»“æ„å’Œæ¨¡å¼"""\n        logger.info("ğŸ” å¼€å§‹åˆ†æä»£ç åº“...")\n\n        try:\n            # ä½¿ç”¨agentçš„å†…éƒ¨ç»„ä»¶è¿›è¡Œåˆ†æ\n            task_planner = self.agent.task_planner\n\n            # åˆ†æé¡¹ç›®ç»“æ„\n            project_info = await task_planner._analyze_project_structure()\n\n            # æ£€ç´¢å…³é”®ä»£ç æ¨¡å¼ï¼ˆä½¿ç”¨é€šç”¨æŸ¥è¯¢ï¼‰\n            common_patterns = await task_planner._retrieve_relevant_code(\n                "class function implementation pattern"\n            )\n\n            analysis_result = {\n                "project_structure": project_info,\n                "common_patterns": common_patterns,\n                "analysis_timestamp": asyncio.get_event_loop().time(),\n                "repo_path": self.repo_path,\n            }\n\n            logger.info(f"âœ… ä»£ç åº“åˆ†æå®Œæˆ")\n            logger.info(f"   ğŸ“ æ€»æ–‡ä»¶æ•°: {project_info.get(\'total_files\', 0)}")\n            logger.info(\n                f"   ğŸ”¤ ä¸»è¦è¯­è¨€: {\', \'.join(project_info.get(\'main_languages\', [])[:3])}"\n            )\n            logger.info(f"   ğŸ“‹ å‘ç°æ¨¡å¼: {len(common_patterns)} ä¸ª")\n\n            return analysis_result\n\n        except Exception as e:\n            logger.error(f"âŒ ä»£ç åº“åˆ†æå¤±è´¥: {str(e)}")\n            return {"success": False, "error": str(e)}', 'similarity': 0.8}]}
